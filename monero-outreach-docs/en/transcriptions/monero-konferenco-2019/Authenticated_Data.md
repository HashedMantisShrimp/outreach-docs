Thanks, Brandon. Okay, so, let's see, this is the clicker. Ah, green. Got it. No, that's the green arrow, perfect.

So, okay, blockchains are transactional databases. So naturally, as with any transactional database, they grow in size with use but the significant thing about, you know, blockchains like Bitcoin or Monero is that they're supposed to be decentralized in their maintenance so they aim to have many many participants called miners all be able to verify transactions that update the database and as the size of the database grows then the size, the amount of storage, that that all the participants in the system need to maintain in order to verify transactions grows as well, and if it grows to be too large —to some degree— it inhibits the goals of decentralization because we want to make it as easy as possible for anyone to be able to participate in consensus not necessarily using a tremendous amount of storage. You could imagine a future where even a SmartWatch or something very with very very little memory to still be able to participate in verification of the entire blockchain, right. 

So let's start with a very simplified view of what blockchain consensus looks like in order to explain what I'll call the difference between stateful consensus versus stateless consensus, which will have involve minimal storage among the miners participating in consensus. So, in stateful consensus transactions come in and miners all listen to the transactions they all hand the transactions to their log and so everybody is storing a copy of this database, now, of course, that could be great because highly high replication is a good thing for fault tolerance but you might not want to design a system where every single person participating in consensus absolutely needs to replicate the entire log of transactions.

So the question is, you know, what can we do to, first of all, so, how do these, how do, how do we handle miners comparing that they are all talking about the same database without sending the entire database to each other? Well, we know that's easy, right, we just could compute a hash of the database and compare efficiently. Now that's not actually how it's done because we also want, and this is a little of a form of stateless verification, we want a miner, who, you know, who is not storing the entire transaction log and able to compute a big hash over everything to be able to just store what's called the state head and be able to immediately verify that every other miner in the system is still talking about the same database. 

Let's say this miner is only listening to transactions as they happen —not storing the whole database—, how can that miner verify everyone else is still talking about the same database? Well, we just define the hash in a way that can be computed in a streaming way, and that's and that's like this, where the hash gets updated by hashing in the previous hash head with a new update and that's where we get our, you know, our blockchain.

So, what happens now when we add rules to this database? Well, then somebody who's just listening to the system, maybe they can verify that everybody else still agrees about the same sequence of updates all the same transaction log but certainly, this miner is participating in the system with no storage has no way of verifying that new transactions coming in, are actually correct or not, right, for example, one rule is, well, this new transaction spends the same coin that a previous transaction spent so that transaction should be rejected but this miner with no storage has no way of knowing this. So what we would like from stateless verification is the ability of a miner with very, very, very small compact storage —like just storing a little hash representation of everything that's happened so far— would be able to, you know, listen to transactions coming in and say, "up", this should be rejected, or "up" this should be accepted.

So how could we possibly do that? How could we possibly define these compact hashes in a way that, you know, all the nodes can just store this hash and still be able to check the validity of transactions? Well, clearly we need something additional, okay, some kind of additional proof that will be sent along with transactions so we'll make the transaction slightly more complicated but what I'm going to talk about is, well, how is that proof generated? How efficient is it? And, who generates the proof? First of all. But to take a step back, just from maybe a philosophical perspective, or you know a more abstract view, what we're calling for is a separation between the consensus layer of the blockchain and the state storage of the blockchain. And both of these we want to be decentralized or distributed and fault-tolerant but we can really look at them as two different systems and one which aims to, you know, maintain the and store all the data in the blockchain and be able to provide basically proofs and state updates to the consensus which is really not focused on necessarily providing a really high-performance database but rather just being able to participate in agreement and verify that things look good.

So, let's focus on a concrete now —it's like a specific example of a blockchain model— and, so I'll focus on UTXOs, as since it's relevant to both Bitcoin and Monero. 

So, what are UTXOs? If in case, you know, just as a reminder or whatever, every transaction consumes inputs, think of like coins assigned addresses that are the inputs and transactions. Every transaction creates outputs, coins are reassigned to new addresses and UTXOs are simply the transaction outputs that have not been consumed yet, so they can still be spent, they're still valid inputs to new transactions. And that's really what the miners all the miners really need to agree on —what are the set of records which are still valid? And that's all you really need to know in order to be able to verify the correctness of a transaction, aside from other things that don't depend on the state of the database, like the transaction is well-formatted.

So, this small compact hash thing, that I was talking about, it's going to be some kind of commitment to the UTXO set and we'll talk about how to update that commitment dynamically and how to verify proofs that something is in this set or not. And the general cryptographic technique that is needed for this is called an accumulator. We all know, or, maybe some of us know, of like the most popular example of an accumulator which is a Merkel tree —and I'll talk about that in a second—, and this kind of work is about doing things that are better than Merkel trees for this task, but abstractly what the accumulator does is it allows you to give a short proof that say all the UTXOs in the transaction are actually part of the UTXO set.

Okay, so let's say we have this kind of simplified picture of a blockchain then the proposal for these UTXOs commitments which was actually, you know, proposed a considerable time ago, was to say, "Okay, well, let's say every block can basically keep track of, consensus can keep track of, what is this like small hash commitment, basically to, you know, all the UTXOs." And so let's just imagine that we have a way to do that —we'll talk about in a lot more detail how that can possibly be done— but let's say consensus is able to keep track of a this accumulator hash that represents with very very little storage the UTXO set and that it's possible to be able to prove that something is in there. So one way to do that is to use a Merkel tree where this —the UTXO commitment— is simply the root of a Merkel tree over all the UTXOs, and the transactions would then basically provide inclusion proofs which can be done with a log in size path up the Merkel tree, and so every transaction would basically include a Merkel proof that the reference UTXO in the transaction is inside this Merkel tree and miners would say, "Okay, this looks good", but they don't actually need to store the whole UTXO set.

So, stepping back and looking more broadly at accumulators. So accumulators —like what we didn't talk about what that Merkel tree example was— also, you know, how do things get added to the accumulator? How do things get added to the Merkel tree? Can we do that in a distributed way so that people who are not storing the entire state of the accumulator and they see a new transaction, can all simultaneously update the accumulator to get a new state, so update the root of the Merkel tree, you can think of? 

So Merkel trees are examples of accumulators. There's also RSA accumulators which is the basis of the work that I'm going to present in this talk, and there's other types of accumulators, they all achieve different kinds of trade-offs. One nice thing about RSA accumulators and these pairing-based accumulators or that the size of the proof is constant size as opposed to log size, which the Merkel trees have.

So, let's say we were using Merkel tree witnesses for this, you know, for, let's say we were using Merkel tree witnesses for these transaction proofs. Well, then let's say that we had a 100 million UTXOs, every Merkel tree witness, you know, while one witness doesn't seem that large together they get to be quite large right, and so it requires about eight hundred bytes per UTXO. And, so, that's a lot of extra storage to keep around, so even though we've reduced the storage of the miners in the system —who now only need to store the root of this Merkel tree— every transaction is going to have this Merkel tree proof and since many transactions not only reference one UTXO but many many UTXOs, whereas before those UTXOs could just be, a like, 64-bit index into the minor storage —that they already have— referencing something they already have, now it has to include this large Merkel witness, so, the communication in the system could potentially blow up by like a factor 100. So, that's not good, and the desiderata for our design desiderata are not only to have short membership witnesses, better efficiently updatable, etc. but also to be able to aggregate witnesses and even better to be able to batch generate and to verify them, efficiently in a batch. So, you could imagine a transaction that references a 100 UTXOs and just have to have one short membership witness that accounts for them all, and/or miners would be able to take many different transactions and verify in a batch all the membership witnesses.

So that's our goal in this current work and the starting point for our new accumulator construction that has these properties is RSA accumulators. 

So very briefly, how does an RSA accumulator work? There is a trusted setup —and I'll talk later about how to do this without trusted setup— but the RSA accumulator is based on an RSA modulus with an unknown factorization so N equals p times q, p and q are secret primes.

Adding something to the accumulator is just taking the hash of that thing and taking the current state of the accumulator, call it Ai, and raising it to the power of the hash of that element. So you can think of after we add an entire set to the accumulator what the accumulator looks like is it's g to the u, g is the initial state just, you know, some generator in ZN these are numbers modulo N and u would be a product of the hashes of all the elements that are in the set, right. So generating this accumulator commitment to the entire set amounts to multiplying the hashes of all the elements in the set together getting basically, and then raising g to the u, and so, g to the u is just one integer modulo N. So it's just one constant size integer.

And deleting an element from the accumulator would amount to canceling out the element we want to delete from the exponent, so if we have some way of getting Ai to the 1 over H of x, then we can delete from the accumulator and we'll talk about how that could be done.

So what's a membership witness in this case? Well, a membership witness is supposed to prove that, you know, that your hash of your element was included, and for now I'm just going to ignore the hash and just talk about the element that you're, the member of the accumulator that you're trying to prove is x, and x is already the hash of the actual element that you're, you know, that's in the accumulator. So you can think of x as the hash of your UTXO. So if you can provide A to the 1 over x, namely a value pi such that pi to the x is equal to the state of the accumulator, then you've shown that x is in the big product in the exponent of the accumulator. And in order to prove that something is not in the accumulator you want to show that x does not appear in this big product in the exponent, in other words, x is co-prime in fact with the product of elements in the accumulator. One thing it was on the slide but I didn't mention it verbally was that this special hash function maps things to primes so the output of every hash is actually a prime number and that guarantees that if we haven't added an element to the accumulator yet then it will be co-prime with that big exponent in the accumulator. And so then we're able to generate these efficient non-membership witnesses which can be easily verified because of this trick which finds these coefficients a b such that a x plus b u is equal to 1, and that can only be done if x is co-prime with u.

So there's also other things to do, stateless updates, I won't get into that.

How can we aggregate membership witnesses? We have one membership witness pi1 such that pi1 to the x is equal to the accumulator state. We have pi2, such that pi2 to the y is equal to the accumulator state. So then we can use this thing called Shamir's trick, which also generates these, you know, fancy coefficients a and b such that you get this relation a x plus b y is equal to 1, and if we just produce the membership witness pi, pi1, 2, which is pi1 to the b times pi2 to the a that actually is an aggregate membership witness and that means that all membership witnesses per transaction block can just be represented with like 3,000 bits, right. So you'd have a transaction that references many many many UTXOs instead of having their membership proof blow up per UTXO, as with Merkel tree proofs. With accumulators, we can just give one single-size membership proof.

Cool, so that was one of our goals and let's talk about deletions. So unfortunately deleting things requires knowledge of the factorization of p and q naively. However, and we definitely don't want to do that cause nobody should know the factorization otherwise, you know, the system is broken, but a cool thing is that you can observe that if you have the membership proof for an item then you can actually delete the item because the state of the accumulator after deleting is simply the membership proof for that thing because you already had the 1 over x root, that was the membership witness. So if you have the membership witness —and that was already generated for your UTXO—, then after you spend it, then you can broadcast that and say, "Okay, this is the new state of the accumulator". In fact, all the miners who saw the membership witness now know the state of the accumulator without that element. Of course, though, transactions happen in parallel so we would want a way of taking many many many membership witnesses for different items and finding a way to combine them in order to batch delete multiple items and that's something that we show how to do.

So that means that for this blockchain application the miners who are listening to the system and —I'll talk, you know, give a better visualization of this— are able to update the state of the accumulator when transactions happen to both, add new transaction outputs to the UTXO set which is represented by the accumulator or delete previous now spent transaction outputs from the accumulator all without knowing the trapdoor or any, you know, trapdoor information.

So what about trusted setup? We don't like trusted setup, so well, one potential thing to do is just choose a large unfactorable N —but that wouldn't have to be really huge. Another thing is to choose something that hasn't been broken in a long time, so like, or choose some cryptographer like Ron Rivest who maybe people trust who has, you know, he claims he forgot the factorization of this one particular RSA modulus, maybe you can use that, but another thing you can do is use class groups, and class groups are basically a drop-in replacement for RSA groups because they're a group with where the order is unknown. If you know what that means that's great, otherwise, don't worry about it. It's just the class groups basically have the key property that you need from RSA groups to make these accumulators secure and they're a bit more complicated to work with but we already have some open-source implementations that deal with them. So that's pretty cool.

So the verification of witnesses takes a long time because it does group exponentiations, 2500 exponentiations per second. So that would be pretty slow if you're trying to do a big full sync of the blockchain. So, what can we do to improve that? Well, we'll use this trick called a proof of exponentiation. It's based on a trick that was used for these verifiable delay functions —if you've heard of them— but basically, it's a way of proving efficiently that x to the alpha is equal to y without having the verifier to actually do this exponentiation themselves. What the verifier ends up doing is just reducing alpha modulo —some small 128-bit prime that's chosen randomly in the protocol— and so you might say, "well, asymptotically then that's the same thing", reducing alpha mod l is asymptotically the same complexity as taking an exponentiation x to the alpha is equal to y but in practice it's about 5,000 times faster.

So, what does the security based on? It's based on this complicated thing called an Adaptive Root Assumption. Actually, it's not that complicated, it's basically saying that if I have an element of the group, so, whether RSA group or a class group and I choose a random prime, then it's difficult for me to output a 1 over l root, meaning something which raises to the l power it gives me back that thing that I chose w. And Adaptive Root is what's needed for this thing to be secure. We have a theorem which says that holds in generic groups. Nothing is actually a generic group so that may not mean too much but cryptographers like to do that.

So, what does this look like now with, put together with our, you know, basic blockchain system? Blockchain has a header, transactions, it has a spend set, things that are being spent, it has a new set, and things that are being created, and a bunch of signatures. We have a previous state of the accumulator which represents the current unspent set, sorry, not the spent set, the UTXO set is the unspent set, so what we'll want to do is we'll want to batch delete the spent set S, from the accumulator, and batch add the new set of newly created UTXOs from, into the accumulator —we also have batching for adds, I didn't talk about that but it's a nice trick —and then in the end, what these two things produce are just a bunch of proofs of exponentiation. In fact, one proof of exponentiation for the entire batch delete, one proof of exponentiation for the entire batch add, and this encompasses the membership witnesses as well. So what the miners end up verifying are just two proofs of exponentiation. Which is basically just like two reductions modulo and 128-bit prime, and then verifying the signatures.

So it's quite light in terms of verification on the miner.

Take this with a huge grain of salt, since, you know, I mostly do things on paper, but, you know, back of the envelope Merkle trees you can do about 100,000 adds or verifies per second. Add would be like adding something, adding a UTXO to the accumulator. Verifying would be verifying a membership witness. They're about the same complexity, in terms of the operations you're doing, you're evaluating a bunch of SHA-256 hashes. For RSA Accumulator Add, basically what you have to do one exponentiation per add, even with that chat, and, you know, just taking numbers on my laptop that comes to about like 600 per second and for verification though because of these tricks which give us this amortization and really, each verification is just a few reductions modulo 128-bit prime, that's quite fast, you can do about 20,000 of those per second.

How am I on time? Three minutes, okay.

So very briefly we can generalize this from UTXO type systems to account tracking systems, so systems designed more in the in theorem style —which tracked account balances— and what we would be using there instead of accumulators are vector commitments.

Merkle trees are also vector commitments, in fact, but RSA accumulators are not. So for a vector commitment, you need to be able to prove that the item is not only in the set but it's in the set at a particular position and for a key-value store what you need is actually a very a sparse vector commitment. So you need to be able to prove that, you know, the vector at this position keyed by this value, which is a very very long vector, is equal to this particular value. And then, you would do all the same kind of tricks you have a way of proving that something at a particular key index of the key-value store is equal to some balance b1, and something is equal to b2, and then you can locally verify the rest of the transaction.

So how do we do that? Well, you could use Merkel trees but they have kind of the same issues we talked about before. There's other previous things that were constructed for vector commitments but they have extremely large setup parameters and so in fact for the key-value store it would just be [inaudible] unfeasible because you basically have to have setup parameters that are linear in the length of the vector. So what we do is we have built something that has constant size —common parameters— and still retains these short proofs but something is a certain value at a particular index and we can do that just by using the same accumulator tricks we used. Just basically we start with a bit vector and we represent each bit as, you know, it's 1 or 0, so it's either in the accumulator or not and because we have these batching techniques we can prove a whole bunch of bits at once in one single constant size proof and so that like immediately gives us these constant openings for vector commitments with basically no large setup parameters because the RSA commitment didn't have any large set of parameters.

So, takeaway points. Shifting work from miners to users. We put a little bit more burden on the users to keep track of membership witnesses. It's a whole big debate whether that's reasonable to ask users to do or not but you could imagine also services which, you know, and many, you know, distributed competing services which do this for users, so you could also think of this as basically a more load-balanced diverse sort of system of participants who are playing different roles for the system. So ones that are participating in consensus, ones that are storing the state in the load-balanced distributed way, being able to provide membership witnesses to users, then which then gets passed off to the consensus layer which can really involve anyone running on any kind of set up.

So that is all I have to talk about today. A bunch of references and there's a paper and also an implementation by Cambrian Labs. Thank you [Applause]
